{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python commonFriend.py soc-LiveJournal1Adj.txt\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def mapper(line):\n",
    "    user = line[0]\n",
    "    friends = line[1].split(',')\n",
    "    out = []\n",
    "\n",
    "    for friend in friends:\n",
    "        if len(friend) > 0:\n",
    "            key = ','.join(sorted([user, friend]))\n",
    "            val = set(friends)\n",
    "            out.append((key, val))\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        sys.argv[1] = \"soc-LiveJournal1Adj.txt\"\n",
    "        # sys.exit(-1)\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    result = sc.textFile(sys.argv[1]).map(lambda x: x.split('\\t'))\\\n",
    "            .flatMap(mapper)\\\n",
    "            .reduceByKey(lambda l1, l2: l1.intersection(l2)).sortByKey()\\\n",
    "            .filter(lambda out: len(out[1]) > 0)\\\n",
    "            .map(lambda x: x[0]+\"\\t\"+','.join(sorted(x[1])))\n",
    "    result.repartition(1).saveAsTextFile(\"Q1_Output\")\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = sc.parallelize([(\"cat\",1),(\"dog\", 1),(\"cat\", 2)])\n",
    "reduced = pets.reduceByKey(lambda x, y: x + y)\n",
    "print(reduced.collect())\n",
    "grouped = pets.groupByKey().mapValues(list)\n",
    "#[('cat', [1, 2]), ('dog', [1])] =>[1, 2] is iterable obj need to be map\n",
    "# .mapValues(list) == .map(lambda x : (x[0], list(x[1])))\n",
    "print(grouped.collect())\n",
    "#print(pets.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate();\n",
    "lines = sc.parallelize([(\"to be or\"),(\"not to be\")])\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word : (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    key = line[0]\n",
    "    val = []\n",
    "    \n",
    "    for i in line[1]:\n",
    "        val.append(list(i))\n",
    "            \n",
    "    \n",
    "    return (key,val)\n",
    "\n",
    "visits = sc.parallelize([(\"index.html\", \"1.2.3.4\"),(\"about.html\", \"3.4.5.6\"),(\"index.html\", \"1.3.3.1\") ])\n",
    "pageNames = sc.parallelize([ (\"index.html\", \"Home\"),(\"about.html\", \"About\") ])\n",
    "\n",
    "print(visits.join(pageNames).collect())\n",
    "print(visits.cogroup(pageNames).map(mapper).collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python yelpUserRate.py business.csv review.csv user.csv\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: wordcount <business.csv> <review.csv> <user.csv>\", file=sys.stderr)\n",
    "        sys.argv = [0,0,0,0]\n",
    "        sys.argv[1] = \"business.csv\"\n",
    "        sys.argv[2] = \"review.csv\"\n",
    "        sys.argv[3] = \"user.csv\"\n",
    "        #sys.exit(-1)\n",
    "\n",
    "\n",
    "    sc = SparkContext.getOrCreate();\n",
    "    #\"business_id\"::\"full_address\"::\"categories\"\n",
    "    business = sc.textFile(sys.argv[1]).map(lambda x: (x.split('::')[0], x.split('::')[1]))\\\n",
    "            .filter(lambda out: \"Stanford\" in out[1])#business_id, full_address\n",
    "    #\"review_id\"::\"user_id\"::\"business_id\"::\"stars\"\n",
    "    review = sc.textFile(sys.argv[2]).map(lambda x: (x.split('::')[2], (x.split('::')[1], x.split('::')[3])))\n",
    "    #business_id, user_id, stars\n",
    "    \n",
    "    #\"user_id\"::\"name\"::\"url\"\n",
    "    #user = sc.textFile(sys.argv[3])\n",
    "    \n",
    "    \n",
    "    #business_id, (full_address, (user_id, stars))\n",
    "    join = business.join(review)\n",
    "    print(join.collect()[0:1])\n",
    "    #user_id stars\n",
    "    result = join.map(lambda x: '\\t'.join(x[1][1]))\n",
    "    print(result.collect()[0:1])\n",
    "    #result.repartition(1).saveAsTextFile(\"Q3_output\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(business.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf, desc\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"sql_sample\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# data is in the same folder of this script\n",
    "# load data \n",
    "review = sc.textFile(\"review.csv\").map(lambda line: line.split(\"::\")).toDF()\n",
    "review = review.select(review._1.alias('review_id'), review._2.alias('user_id'), review._3.alias('business_id'), review._4.alias('stars'))\n",
    "\n",
    "\n",
    "\n",
    "#review = review.select('business_id', 'user_id').groupBy('business_id').count()\n",
    "#review = review.filter(\"`count` >= 10\").orderBy('count', ascending=False)\n",
    "review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.0, -2.0, 3.0]),),\n",
    "    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),),\n",
    "    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], [\"features\"])\n",
    "\n",
    "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"featuresDCT\")\n",
    "\n",
    "dctDf = dct.transform(df)\n",
    "\n",
    "dctDf.select(\"featuresDCT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([2.0, 1.0]),),\n",
    "    (Vectors.dense([0.0, 0.0]),),\n",
    "    (Vectors.dense([3.0, -1.0]),)\n",
    "], [\"features\"])\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyDF = polyExpansion.transform(df)\n",
    "\n",
    "polyDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
