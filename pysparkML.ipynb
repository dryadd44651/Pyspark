{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans\n",
    "Using spark machine learning library spark-mlib, use kmeans to cluster the\n",
    "movies using the ratings given by the user, that is, use the item-user matrix from\n",
    "itemusermat File provided as input to your program.\n",
    "\n",
    "The itemusermat file contains the ratings given to each movie by the users in Matrix\n",
    "format.\n",
    "\n",
    "The file contains the ratings by users for 1000 movies.\n",
    "\n",
    "Each line contains the movies id and the list of ratings given by the users. \n",
    "\n",
    "A rating of 0 is used for entries where the user did not rate a movie.\n",
    "\n",
    "Below, we show an example of the format of Itemusermat file with the item-user\n",
    "matrix. \n",
    "\n",
    "Note that here, user 1 did not rate movie 2, so we use a rating of 0.\n",
    "\n",
    "items|user1|user2|\n",
    "------|----|-----|\n",
    "movie1|4|3|\n",
    "movie2|0|2|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"kmeans\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#item-user matrix\n",
    "matrix = sc.textFile(\"itemusermat\").map(lambda x: x.split(\" \"))\\\n",
    "                                            .map(lambda x: (x[0], x[1:]))\n",
    "# print(matrix.top(2))\n",
    "\n",
    "# Create dataframe\n",
    "data = [(x[0], Vectors.dense(x[1]), ) for x in matrix.collect()]\n",
    "matrix = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "# print(matrix.show())\n",
    "# Train k-means\n",
    "kmeans = KMeans(k=10, seed=1234)\n",
    "model = kmeans.fit(matrix)\n",
    "\n",
    "# Predict\n",
    "transformed = model.transform(matrix)\n",
    "# print(transformed.show())\n",
    "predictionIndex = transformed.rdd.map(lambda row: (int(row.id), row.prediction))\n",
    "# print(predictionIndex.top(20))\n",
    "\n",
    "# # Join with movieInfo\n",
    "movieInfo = sc.textFile(\"movies.dat\").map(lambda x: x.split(\"::\"))\\\n",
    "                                    .map(lambda x: (int(x[0]), (x[1], x[2])))\n",
    "joined = movieInfo.join(predictionIndex)\\\n",
    "                    .map(lambda x: (x[1][1], (x[1][1], x[0], x[1][0][0], x[1][0][1])))\n",
    "result = joined.groupByKey()\\\n",
    "                .map(lambda x: list(x[1])[0:5])\\\n",
    "                .flatMap(lambda x: x)\\\n",
    "                .sortBy(lambda x: x[0])\\\n",
    "                .collect()\n",
    "sc.parallelize(result).saveAsTextFile(\"kmeans_Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS model.\n",
    "Use Collaborative filtering to find the accuracy of ALS model.\n",
    "\n",
    "For this program, you will use ratings.dat file, which contains:\n",
    "\n",
    "User id :: movie id :: ratings :: timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import math\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"recommend\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "raw = spark.sparkContext.textFile(\"ratings.dat\")\n",
    "# make the data (uid, mid, timstamp, rating) \n",
    "data = raw.map(lambda line: line.split(\"::\"))\\\n",
    "                .map(lambda tokens: (tokens[0], tokens[1], tokens[2]))\n",
    "\n",
    "tranData, testData = data.randomSplit([7, 3], seed=1011)\n",
    "tranData.cache()\n",
    "\n",
    "testDataRdd = testData.map(lambda x:(x[0], x[1]))\n",
    "testDataRdd.cache()\n",
    "\n",
    "seed = 1234\n",
    "iterations = 10\n",
    "regularization = 0.1\n",
    "ranks = [8, 10, 12, 15]\n",
    "errors = [0, 0, 0, 0]\n",
    "idx = 0\n",
    "\n",
    "minErr = float('inf')\n",
    "bestRank = -1\n",
    "\n",
    "for rank in ranks:\n",
    "    model = ALS.train(tranData, rank, seed=seed, iterations=iterations,\n",
    "            lambda_=regularization)\n",
    "    predictions = model.predictAll(testDataRdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    ratesPreds = testData.map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\\\n",
    "                            .join(predictions) \n",
    "    error = ratesPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "    errors[idx] = error\n",
    "    idx += 1\n",
    "    print('Rank %s MSE : %s'%(rank, error))\n",
    "    if(error < minErr):\n",
    "        minErr = error\n",
    "        bestRank = rank\n",
    "\n",
    "print(\"The best rank : %s\" % bestRank) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit9ca157234d374370bd129a27b0c0042b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
